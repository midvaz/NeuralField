{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b28394",
   "metadata": {},
   "source": [
    "# Зависимость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329726da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midv/project/NeuralField/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2d96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        rgb_dir: Path, \n",
    "        mask_dir: Path, \n",
    "        size=512, \n",
    "        augment=False,\n",
    "    ):\n",
    "        self.rgb_paths = sorted(rgb_dir.glob(\"*\"))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.tf = self.build_tf()\n",
    "\n",
    "    def build_tf(self):\n",
    "        tf = [A.Resize(self.size, self.size)]\n",
    "        if self.augment:\n",
    "            tf += [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.ColorJitter(p=0.3),\n",
    "            ]\n",
    "        tf += [A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)), ToTensorV2(transpose_mask=True)]\n",
    "        return A.Compose(tf)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.rgb_paths[idx]\n",
    "        mask_path = self.mask_dir / img_path.with_suffix(\".png\").name\n",
    "\n",
    "        rgb = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        transformed = self.tf(image=rgb, mask=mask)\n",
    "        return transformed[\"image\"].float(), transformed[\"mask\"].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d015bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_metrics(\n",
    "    logits: torch.Tensor, \n",
    "    target: torch.Tensor, \n",
    "    num_classes: int,\n",
    "):\n",
    "    preds = logits.argmax(1)\n",
    "    valid = target != 255\n",
    "    correct = (preds[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    pix_acc = (correct / total).item() if total > 0 else 0.0\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        inter = ((preds == cls) & (target == cls) & valid).sum().item()\n",
    "        union = ((preds == cls) | (target == cls)) & valid\n",
    "        union = union.sum().item()\n",
    "        if union > 0:\n",
    "            ious.append(inter / union)\n",
    "    miou = float(np.mean(ious)) if ious else 0.0\n",
    "    return pix_acc, miou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdccb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, device, num_classes):\n",
    "    model.train()\n",
    "    tot_loss = tot_acc = tot_iou = 0.0\n",
    "    for img, mask in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(pixel_values=img, labels=mask)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        acc, miou = compute_metrics(out.logits.detach(), mask, num_classes)\n",
    "        n = img.size(0)\n",
    "        tot_loss += loss.item() * n\n",
    "        tot_acc += acc * n\n",
    "        tot_iou += miou * n\n",
    "    n_samples = len(loader.dataset)\n",
    "    return tot_loss / n_samples, tot_acc / n_samples, tot_iou / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6822a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    tot_loss = tot_acc = tot_iou = 0.0\n",
    "    for img, mask in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        out = model(pixel_values=img, labels=mask)\n",
    "        loss = out.loss\n",
    "        acc, miou = compute_metrics(out.logits, mask, num_classes)\n",
    "        n = img.size(0)\n",
    "        tot_loss += loss.item() * n\n",
    "        tot_acc += acc * n\n",
    "        tot_iou += miou * n\n",
    "    n_samples = len(loader.dataset)\n",
    "    return tot_loss / n_samples, tot_acc / n_samples, tot_iou / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d7dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(model, path: Path, epoch: int, miou: float):\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"epoch\": epoch, \"miou\": miou}, \n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "def log_header(path: Path):\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow(\n",
    "            [\"epoch\", \"loss\", \"val_loss\", \"val_acc\", \"val_miou\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def log_row(path: Path, row: List):\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2182160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train-dir TRAIN_DIR --val-dir VAL_DIR\n",
      "                             [--num-classes NUM_CLASSES] [--epochs EPOCHS]\n",
      "                             [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                             [--checkpoint-dir CHECKPOINT_DIR]\n",
      "                             [--model-size {b0,b1,b2,b3}]\n",
      "                             [--img-size IMG_SIZE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train-dir, --val-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midv/project/NeuralField/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"SegFormer ViT training on RGB only\")\n",
    "# parser.add_argument(\"--train-dir\", required=True)\n",
    "# parser.add_argument(\"--val-dir\", required=True)\n",
    "# parser.add_argument(\"--num-classes\", type=int, default=9)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "# parser.add_argument(\"--batch-size\", type=int, default=4)\n",
    "# parser.add_argument(\"--lr\", type=float, default=6e-5)\n",
    "# parser.add_argument(\"--checkpoint-dir\", default=\"runs/vit_rgb\")\n",
    "# parser.add_argument(\"--model-size\", choices=[\"b0\", \"b1\", \"b2\", \"b3\"], default=\"b2\")\n",
    "# parser.add_argument(\"--img-size\", type=int, default=512)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# train_rgb = Path(args.train_dir) / \"rgb\"\n",
    "# train_mask = Path(args.train_dir) / \"masks\"\n",
    "# val_rgb = Path(args.val_dir) / \"rgb\"\n",
    "# val_mask = Path(args.val_dir) / \"masks\"\n",
    "\n",
    "train_rgb = Path(\"/mnt/d/Agriculture-Vision-2021 2/train/images/rgb\")\n",
    "train_mask = Path(\"/mnt/d/Agriculture-Vision-2021 2/train/masks\")\n",
    "val_rgb = Path(\"/mnt/d/Agriculture-Vision-2021 2/val/images/rgb\")\n",
    "val_mask = Path(\"/mnt/d/Agriculture-Vision-2021 2/val/masks\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "id2label = {i: f\"class_{i}\" for i in range(args.num_classes)}\n",
    "cfg = SegformerConfig.from_pretrained(\n",
    "    f\"nvidia/mit_{args.model_size}\",\n",
    "    num_channels=3,\n",
    "    num_labels=args.num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id={v: k for k, v in id2label.items()},\n",
    "    ignore_index=255,\n",
    ")\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    f\"nvidia/mit_{args.model_size}\", config=cfg, ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "train_ds = RGBDataset(train_rgb, train_mask, size=args.img_size, augment=True)\n",
    "val_ds = RGBDataset(val_rgb, val_mask, size=args.img_size, augment=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ckpt_dir = Path(args.checkpoint_dir); ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_path = ckpt_dir / \"train.csv\"; log_header(log_path)\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "sched = ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)\n",
    "best_miou = 0.0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{args.epochs}\")\n",
    "    tr_loss, tr_acc, tr_iou = train_epoch(model, train_loader, opt, device, args.num_classes)\n",
    "    val_loss, val_acc, val_iou = eval_epoch(model, val_loader, device, args.num_classes)\n",
    "    sched.step(val_loss)\n",
    "    log_row(log_path, [epoch, tr_loss, val_loss, val_acc, val_iou])\n",
    "    print(f\"loss={tr_loss:.3f}  val_loss={val_loss:.3f}  val_acc={val_acc:.3f}  val_mIoU={val_iou:.3f}\")\n",
    "\n",
    "    if val_iou > best_miou:\n",
    "        best_miou = val_iou\n",
    "        save_ckpt(model, ckpt_dir / \"best_model.pt\", epoch, val_iou)\n",
    "        print(f\"✔ Saved best mIoU {val_iou:.3f} at epoch {epoch}\")\n",
    "\n",
    "print(f\"Training finished. Best mIoU = {best_miou:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839e2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 14:45:58.990234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-22 14:45:59.002471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-22 14:45:59.005481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-22 14:45:59.015753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734867962.435943   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.509371   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.509461   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU доступен и будет использован для обучения.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Убедимся, что TensorFlow использует GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        # os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "        print(\"GPU доступен и будет использован для обучения.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Ошибка настройки памяти GPU: {e}\")\n",
    "else:\n",
    "    print(\"GPU не обнаружен. Используется CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры датасета\n",
    "DATASET_ROOT_DIR = \"/mnt/d/\"\n",
    "DATASET_DIR = os.path.join(DATASET_ROOT_DIR, \"Agriculture-Vision-2021 2\")   # Задайте путь к датасету\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "IMG_SIZE = 512  # Размер входного изображения\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 9  # Количество классов сегментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not os.path.isdir(DATASET_ROOT_DIR):\n",
    "    sys.exit(\"Dataset dir is not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Патчинг изображения\n",
    "# class PatchEmbedding(layers.Layer):\n",
    "#     def __init__(self, patch_size, embed_dim):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.projection = layers.Conv2D(\n",
    "#             filters=self.embed_dim,\n",
    "#             kernel_size=self.patch_size,\n",
    "#             strides=self.patch_size,\n",
    "#             padding=\"valid\"\n",
    "#         )\n",
    "#         self.flatten = layers.Reshape((-1, self.embed_dim))\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.projection(inputs)\n",
    "#         x = self.flatten(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Модель ViT для семантической сегментации\n",
    "# class VisionTransformer(Model):\n",
    "#     def __init__(self, img_size, patch_size, num_classes, embed_dim, depth, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "#         super().__init__()\n",
    "#         self.img_size = img_size\n",
    "#         self.patch_size = patch_size\n",
    "#         self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "#         self.patch_embedding = PatchEmbedding(patch_size, embed_dim)\n",
    "#         self.position_embedding = self.add_weight(\n",
    "#             shape=(1, self.num_patches, embed_dim),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True\n",
    "#         )\n",
    "\n",
    "#         self.transformer_blocks = [\n",
    "#             TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(depth)\n",
    "#         ]\n",
    "\n",
    "#         self.decoder = tf.keras.Sequential([\n",
    "#             layers.Dense(embed_dim),\n",
    "#             layers.Reshape((img_size // patch_size, img_size // patch_size, embed_dim)),\n",
    "#             layers.Conv2DTranspose(filters=embed_dim // 2, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "#             layers.Conv2DTranspose(filters=embed_dim // 4, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "#             layers.Conv2DTranspose(filters=embed_dim // 8, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "#             layers.Conv2D(num_classes, kernel_size=1, activation=\"softmax\")\n",
    "#         ])\n",
    "\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Преобразование в патчи и добавление позиционной эмбеддинга\n",
    "#         x = self.patch_embedding(inputs)\n",
    "#         x += self.position_embedding\n",
    "\n",
    "#         # Пропуск через трансформерные блоки\n",
    "#         for block in self.transformer_blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         # Декодер для восстановления пространственной размерности\n",
    "#         x = self.decoder(x)\n",
    "#         x = tf.image.resize(x, (self.img_size, self.img_size))  # Явно задаем размер выхода\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформерный блок\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim:int, num_heads:int, mlp_dim:int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # многослойный персиптрон \n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=tf.nn.gelu),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout_rate),\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.norm1(inputs + attn_output)\n",
    "\n",
    "        mlp_output = self.mlp(out1, training=training)\n",
    "        return self.norm2(out1 + mlp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size:int, \n",
    "        patch_size:int, \n",
    "        num_classes:int, \n",
    "        embed_dim:int, \n",
    "        depth:int, \n",
    "        num_heads:int, \n",
    "        mlp_dim:int, \n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, embed_dim)\n",
    "        self.position_embedding = self.add_weight(\n",
    "            shape=(1, self.num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(depth)\n",
    "        ]\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Reshape((img_size // patch_size, img_size // patch_size, embed_dim)),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 2, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 4, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 8, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(num_classes, kernel_size=1, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Преобразование в патчи и добавление позиционной эмбеддинга\n",
    "        x = self.patch_embedding(inputs)\n",
    "        x += self.position_embedding\n",
    "\n",
    "        # Пропуск через трансформерные блоки\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Декодер для восстановления пространственной размерности\n",
    "        x = self.decoder(x)\n",
    "        x = tf.image.resize(x, (self.img_size, self.img_size))  # Явно задаем размер выхода\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, img_size:int, patch_size:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = layers.Conv2D(\n",
    "            embed_dim, kernel_size=patch_size, strides=patch_size, padding='valid'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        h, w = self.img_size, self.img_size\n",
    "        if inputs.shape[1] != h or inputs.shape[2] != w:\n",
    "            inputs = tf.image.resize(inputs, (h, w))\n",
    "\n",
    "        x = self.projection(inputs)  # Apply Conv2D to project\n",
    "        x = tf.reshape(x, (batch_size, -1, x.shape[-1]))  # Reshape to (batch_size, num_patches, embed_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Параметры модели\n",
    "PATCH_SIZE = 16 # Размер патча\n",
    "EMBED_DIM = 512  # Увеличенная размерность эмбеддингов\n",
    "DEPTH = 12  # Количество трансформерных блоков\n",
    "NUM_HEADS = 16  # Голов для Multi-Head Attention\n",
    "MLP_DIM = 1024  # Размерность скрытого слоя в MLP\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "PATCH_SIZE = 16  # Было 32\n",
    "EMBED_DIM = 256  # Было 512\n",
    "DEPTH = 6        # Было 12\n",
    "NUM_HEADS = 8    # Было 16\n",
    "MLP_DIM = 512    # Было 1024\n",
    "DROPOUT_RATE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734867962.609212   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.609525   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.609568   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.791520   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734867962.791593   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-22 14:46:02.791603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1734867962.791649   34081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-22 14:46:02.792817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Создание модели\n",
    "vit_segmentation_model = VisionTransformer(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    depth=DEPTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Компиляция модели\n",
    "vit_segmentation_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midv/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'vision_transformer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vision_transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vision_transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ patch_embedding                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_1             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_2             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_3             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_5             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ patch_embedding                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_1             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_2             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_3             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_5             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_6 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m262,144\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,144\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Вывод структуры модели\n",
    "vit_segmentation_model.build((None, IMG_SIZE, IMG_SIZE, 3))\n",
    "vit_segmentation_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение схемы модели\n",
    "plot_model(vit_segmentation_model, to_file=\"vit_model_architecture.png\", show_shapes=True)\n",
    "\n",
    "# Функции для оценки\n",
    "from sklearn.metrics import confusion_matrix, f1_score, jaccard_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, num_classes):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = np.argmax(y_pred, axis=-1).flatten()\n",
    "\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=range(num_classes))\n",
    "    iou = jaccard_score(y_true_flat, y_pred_flat, average=\"macro\")\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average=\"macro\")\n",
    "\n",
    "    mean_accuracy = np.diag(cm).sum() / cm.sum()\n",
    "\n",
    "    return mean_accuracy, iou, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация гистограммы результатов\n",
    "def plot_metrics(metrics, labels):\n",
    "    plt.bar(labels, metrics, color=['blue', 'green', 'red'])\n",
    "    plt.xlabel(\"Метрики\")\n",
    "    plt.ylabel(\"Значения\")\n",
    "    plt.title(\"Оценка модели\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График обучения\n",
    "class TrainingPlotCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history[\"loss\"], label=\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history[\"accuracy\"], label=\"Accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для загрузки данных\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_path(image_path, mask_path):\n",
    "#     img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "#     mask = load_img(mask_path, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"grayscale\")\n",
    "#     img = img_to_array(img) / 255.0\n",
    "#     mask = img_to_array(mask).astype(\"int\")\n",
    "#     return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_path(image_path, mask_path):\n",
    "#     img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "#     mask = load_img(mask_path, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"grayscale\")\n",
    "#     img = img_to_array(img) / 255.0\n",
    "#     mask = img_to_array(mask).astype(\"int\")\n",
    "\n",
    "#     # Удаляем лишнее измерение, если оно есть\n",
    "#     # if len(mask.shape) == 3 and mask.shape[-1] == 1:\n",
    "#     #     mask = tf.squeeze(mask, axis=-1)\n",
    "#     return img, mask\n",
    "\n",
    "# def data_generator(image_dir, mask_dir, limit=6000):\n",
    "#     image_filenames = sorted(os.listdir(image_dir))[:limit]\n",
    "#     mask_filenames = sorted(os.listdir(mask_dir))[:limit]\n",
    "#     for img_filename, mask_filename in zip(image_filenames, mask_filenames):\n",
    "#         img_path = os.path.join(image_dir, img_filename)\n",
    "#         mask_path = os.path.join(mask_dir, mask_filename)\n",
    "#         yield process_path(img_path, mask_path)\n",
    "\n",
    "# def load_dataset(image_dir, mask_dir):\n",
    "#     dataset = tf.data.Dataset.from_generator(\n",
    "#         lambda: data_generator(image_dir, mask_dir),\n",
    "#         output_types=(tf.float32, tf.int32),\n",
    "#         output_shapes=((IMG_SIZE, IMG_SIZE, 3), (IMG_SIZE, IMG_SIZE, 1))\n",
    "#     )\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_is_nune(train_data_dir: list, train_mask_dir: list) -> tuple[str, str]:\n",
    "    count_mask = 0 \n",
    "    count_image = 0 \n",
    "    for img_path, mask_path in zip(train_data_dir, train_mask_dir):\n",
    "        if not os.path.isfile(img_path):\n",
    "            count_image += 1\n",
    "            print(f\"Файл изображения не найден: {img_path}\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            count_mask += 1\n",
    "            print(f\"Файл маски не найден: {mask_path}\")\n",
    "    \n",
    "    return f\"Количество ошибок масок = {count_mask}\", f\"Количество ошибок изображений = {count_image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_image_mask(image_path, mask_path):\n",
    "    \"\"\"Обрабатывает пары изображений и масок.\"\"\"\n",
    "    # Чтение изображения и маски\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])  # ошибка ? \n",
    "    img = tf.cast(img, tf.float32) / 255.0  # Нормализация\n",
    "\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_image(mask, channels=1)\n",
    "    mask.set_shape([None, None, 1])\n",
    "    mask = tf.image.resize(mask, [IMG_SIZE, IMG_SIZE], method=\"nearest\")  # Для масок используем nearest\n",
    "    mask = tf.cast(mask, tf.int32)  # Целочисленные значения для меток\n",
    "\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "def load_dataset(image_dir, mask_dir, limit=None, batch_size=32, shuffle=True):\n",
    "    \"\"\"Загружает датасет из директорий изображений и масок.\"\"\"\n",
    "    # Получение путей к изображениям и маскам\n",
    "    print(\"я в load_dataset\")\n",
    "    \n",
    "    count_err = 0\n",
    "    try:\n",
    "        image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
    "        mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
    "    except Exception as e:\n",
    "        count_err += 1\n",
    "        print(e)\n",
    "        \n",
    "    print(f\"{count_err = }\")\n",
    "    \n",
    "    if limit:\n",
    "        image_paths = image_paths[:limit]\n",
    "        mask_paths = mask_paths[:limit]\n",
    "        \n",
    "    a, b = count_is_nune(image_paths, mask_paths)\n",
    "    \n",
    "    print(a)\n",
    "    print(b)\n",
    "    \n",
    "    # Создание TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "    dataset = dataset.map(parse_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths), reshuffle_each_iteration=True)\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "train_data_dir = os.path.join(TRAIN_DIR, \"images\", \"rgb\")\n",
    "train_mask_dir = os.path.join(TRAIN_DIR, \"masks\")\n",
    "\n",
    "valid_data_dir = os.path.join(VAL_DIR, \"images\", \"rgb\")\n",
    "valid_mask_dir = os.path.join(VAL_DIR, \"masks\")\n",
    "\n",
    "if not os.path.isdir(DATASET_ROOT_DIR) \\\n",
    "    or not os.path.isdir(train_data_dir) \\\n",
    "    or not os.path.isdir(train_mask_dir) \\\n",
    "    or not os.path.isdir(valid_data_dir) \\\n",
    "    or not os.path.isdir(valid_mask_dir):\n",
    "        sys.exit(\"Dataset dir is not exist\")\n",
    "else: \n",
    "    print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img_path, mask_path in zip(valid_data_dir, valid_mask_dir):\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         print(f\"Файл изображения не найден: {img_path}\")\n",
    "#     if not os.path.isfile(mask_path):\n",
    "#         print(f\"Файл маски не найден: {mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я в load_dataset\n",
      "count_err = 0\n",
      "Количество ошибок масок = 0\n",
      "Количество ошибок изображений = 0\n",
      "(512, 512, 3)\n",
      "(512, 512, 1)\n",
      "я в load_dataset\n",
      "count_err = 0\n",
      "Количество ошибок масок = 0\n",
      "Количество ошибок изображений = 0\n",
      "(512, 512, 3)\n",
      "(512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(train_data_dir, train_mask_dir, limit=1000, batch_size=16)\n",
    "val_dataset = load_dataset(valid_data_dir, valid_mask_dir, limit=1000, batch_size=16)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Динамический вывод и сохранение модели\n",
    "class TrainingPlotCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Обновляем историю\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        self.history[\"val_accuracy\"].append(logs.get(\"val_accuracy\"))\n",
    "        \n",
    "        # График тренировки\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Потери\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history[\"loss\"], label=\"Train Loss\", marker='o')\n",
    "        plt.plot(self.history[\"val_loss\"], label=\"Val Loss\", marker='o')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Точность\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history[\"accuracy\"], label=\"Train Accuracy\", marker='o')\n",
    "        plt.plot(self.history[\"val_accuracy\"], label=\"Val Accuracy\", marker='o')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели после каждой эпохи\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling PatchEmbedding.call().\n\n\u001b[1m'images' must have either 3 or 4 dimensions.\u001b[0m\n\nArguments received by PatchEmbedding.call():\n  • inputs=tf.Tensor(shape=(None, None, 512, 512, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m vit_segmentation_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     train_dataset,\n\u001b[1;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[TrainingPlotCallback(), checkpoint_callback]\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mVisionTransformer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Преобразование в патчи и добавление позиционной эмбеддинга\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding(inputs)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Пропуск через трансформерные блоки\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mPatchEmbedding.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m h \u001b[38;5;129;01mor\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m w:\n\u001b[0;32m---> 55\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(inputs, (h, w))\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(inputs)  \u001b[38;5;66;03m# Apply Conv2D to project\u001b[39;00m\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(x, (batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))  \u001b[38;5;66;03m# Reshape to (batch_size, num_patches, embed_dim)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling PatchEmbedding.call().\n\n\u001b[1m'images' must have either 3 or 4 dimensions.\u001b[0m\n\nArguments received by PatchEmbedding.call():\n  • inputs=tf.Tensor(shape=(None, None, 512, 512, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "history = vit_segmentation_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[TrainingPlotCallback(), checkpoint_callback]\n",
    ")\n",
    "\n",
    "# # Обучение модели\n",
    "# history = vit_segmentation_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=50,\n",
    "#     callbacks=[TrainingPlotCallback()]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования метрик на валидационном наборе\n",
    "val_images, val_masks = next(iter(val_dataset))\n",
    "val_predictions = vit_segmentation_model.predict(val_images)\n",
    "metrics = evaluate_model(val_masks.numpy(), val_predictions, NUM_CLASSES)\n",
    "plot_metrics(metrics, [\"Mean Accuracy\", \"IoU\", \"F1-Score\"])\n",
    "\n",
    "print(\"Обучение завершено. Схема модели сохранена в 'vit_model_architecture.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

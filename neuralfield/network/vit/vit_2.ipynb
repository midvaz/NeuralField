{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Убедимся, что TensorFlow использует GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "        print(\"GPU доступен и будет использован для обучения.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Ошибка настройки памяти GPU: {e}\")\n",
    "else:\n",
    "    print(\"GPU не обнаружен. Используется CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры датасета\n",
    "DATASET_DIR = \"/mnt/d/Agriculture-Vision-2021 2\"   # Задайте путь к датасету\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "IMG_SIZE = 512  # Размер входного изображения\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 9  # Количество классов сегментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Патчинг изображения\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.projection = layers.Conv2D(\n",
    "            filters=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            strides=self.patch_size,\n",
    "            padding=\"valid\"\n",
    "        )\n",
    "        self.flatten = layers.Reshape((-1, self.embed_dim))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.projection(inputs)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформерный блок\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=tf.nn.gelu),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout_rate),\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.norm1(inputs + attn_output)\n",
    "\n",
    "        mlp_output = self.mlp(out1, training=training)\n",
    "        return self.norm2(out1 + mlp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Модель ViT для семантической сегментации\n",
    "class VisionTransformer(Model):\n",
    "    def __init__(self, img_size, patch_size, num_classes, embed_dim, depth, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(patch_size, embed_dim)\n",
    "        self.position_embedding = self.add_weight(\n",
    "            shape=(1, self.num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(depth)\n",
    "        ]\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Reshape((img_size // patch_size, img_size // patch_size, embed_dim)),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 2, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 4, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(filters=embed_dim // 8, kernel_size=2, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(num_classes, kernel_size=1, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Преобразование в патчи и добавление позиционной эмбеддинга\n",
    "        x = self.patch_embedding(inputs)\n",
    "        x += self.position_embedding\n",
    "\n",
    "        # Пропуск через трансформерные блоки\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Декодер для восстановления пространственной размерности\n",
    "        x = self.decoder(x)\n",
    "        x = tf.image.resize(x, (self.img_size, self.img_size))  # Явно задаем размер выхода\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры модели\n",
    "PATCH_SIZE = 16 # Размер патча\n",
    "EMBED_DIM = 512  # Увеличенная размерность эмбеддингов\n",
    "DEPTH = 12  # Количество трансформерных блоков\n",
    "NUM_HEADS = 16  # Голов для Multi-Head Attention\n",
    "MLP_DIM = 1024  # Размерность скрытого слоя в MLP\n",
    "DROPOUT_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели\n",
    "vit_segmentation_model = VisionTransformer(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    depth=DEPTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Компиляция модели\n",
    "vit_segmentation_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод структуры модели\n",
    "vit_segmentation_model.build((None, IMG_SIZE, IMG_SIZE, 3))\n",
    "vit_segmentation_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение схемы модели\n",
    "plot_model(vit_segmentation_model, to_file=\"vit_model_architecture.png\", show_shapes=True)\n",
    "\n",
    "# Функции для оценки\n",
    "from sklearn.metrics import confusion_matrix, f1_score, jaccard_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, num_classes):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = np.argmax(y_pred, axis=-1).flatten()\n",
    "\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=range(num_classes))\n",
    "    iou = jaccard_score(y_true_flat, y_pred_flat, average=\"macro\")\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average=\"macro\")\n",
    "\n",
    "    mean_accuracy = np.diag(cm).sum() / cm.sum()\n",
    "\n",
    "    return mean_accuracy, iou, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация гистограммы результатов\n",
    "def plot_metrics(metrics, labels):\n",
    "    plt.bar(labels, metrics, color=['blue', 'green', 'red'])\n",
    "    plt.xlabel(\"Метрики\")\n",
    "    plt.ylabel(\"Значения\")\n",
    "    plt.title(\"Оценка модели\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График обучения\n",
    "class TrainingPlotCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history[\"loss\"], label=\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history[\"accuracy\"], label=\"Accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для загрузки данных\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_path(image_path, mask_path):\n",
    "#     img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "#     mask = load_img(mask_path, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"grayscale\")\n",
    "#     img = img_to_array(img) / 255.0\n",
    "#     mask = img_to_array(mask).astype(\"int\")\n",
    "#     return img, mask\n",
    "def process_path(image_path, mask_path):\n",
    "    img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    mask = load_img(mask_path, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"grayscale\")\n",
    "    img = img_to_array(img) / 255.0\n",
    "    mask = img_to_array(mask).astype(\"int\")\n",
    "\n",
    "    # Удаляем лишнее измерение, если оно есть\n",
    "    # if len(mask.shape) == 3 and mask.shape[-1] == 1:\n",
    "    #     mask = tf.squeeze(mask, axis=-1)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_dir, mask_dir):\n",
    "    image_filenames = sorted(os.listdir(image_dir))\n",
    "    mask_filenames = sorted(os.listdir(mask_dir))\n",
    "    for img_filename, mask_filename in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        yield process_path(img_path, mask_path)\n",
    "\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(image_dir, mask_dir),\n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=((IMG_SIZE, IMG_SIZE, 3), (IMG_SIZE, IMG_SIZE, 1))\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(os.path.join(TRAIN_DIR, \"images/rgb\"), os.path.join(TRAIN_DIR, \"masks\"))\n",
    "val_dataset = load_dataset(os.path.join(VAL_DIR, \"images/rgb\"), os.path.join(VAL_DIR, \"masks\"))\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Динамический вывод и сохранение модели\n",
    "class TrainingPlotCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Обновляем историю\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        self.history[\"val_accuracy\"].append(logs.get(\"val_accuracy\"))\n",
    "        \n",
    "        # График тренировки\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Потери\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history[\"loss\"], label=\"Train Loss\", marker='o')\n",
    "        plt.plot(self.history[\"val_loss\"], label=\"Val Loss\", marker='o')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Точность\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history[\"accuracy\"], label=\"Train Accuracy\", marker='o')\n",
    "        plt.plot(self.history[\"val_accuracy\"], label=\"Val Accuracy\", marker='o')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Сохранение модели после каждой эпохи\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"model_epoch_{epoch:02d}.keras\",\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "history = vit_segmentation_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[TrainingPlotCallback(), checkpoint_callback]\n",
    ")\n",
    "\n",
    "# # Обучение модели\n",
    "# history = vit_segmentation_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=50,\n",
    "#     callbacks=[TrainingPlotCallback()]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования метрик на валидационном наборе\n",
    "val_images, val_masks = next(iter(val_dataset))\n",
    "val_predictions = vit_segmentation_model.predict(val_images)\n",
    "metrics = evaluate_model(val_masks.numpy(), val_predictions, NUM_CLASSES)\n",
    "plot_metrics(metrics, [\"Mean Accuracy\", \"IoU\", \"F1-Score\"])\n",
    "\n",
    "print(\"Обучение завершено. Схема модели сохранена в 'vit_model_architecture.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
